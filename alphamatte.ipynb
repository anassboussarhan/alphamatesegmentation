{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled0.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anassboussarhan/alphamatesegmentation/blob/master/alphamatte.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "SnmAkbQF4_VM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.python.lib.io import file_io\n",
        "import os"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "e4GbbvCJ5utK",
        "colab_type": "code",
        "outputId": "684555f1-c1da-4b21-e455-aeafc63e1b9c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ELz31p_65AQe",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "FUNCTIONS"
      ]
    },
    {
      "metadata": {
        "id": "1T3KCKdGirkf",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def conv(x, name, filters, kernel_size=3, strides=1, dilation=1):\n",
        "    with tf.variable_scope(name):\n",
        "        x = tf.layers.conv2d(x, filters=filters, kernel_size=kernel_size, strides=strides,\n",
        "                             dilation_rate=dilation, padding=\"same\")\n",
        "    return x\n",
        "\n",
        "\n",
        "def instance_norm(x, name, epsilon=1e-5):\n",
        "    with tf.variable_scope(name):\n",
        "        gamma = tf.get_variable(initializer=tf.ones([x.shape[-1]]), name=\"gamma\")\n",
        "        beta = tf.get_variable(initializer=tf.zeros([x.shape[-1]]), name=\"beta\")\n",
        "        mean, var = tf.nn.moments(x, axes=[1,2], keep_dims=True)\n",
        "        x = tf.nn.batch_normalization(x, mean, var, beta, gamma, epsilon, name=\"norm\",)\n",
        "    return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "PbA5wyfBiztz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def loss_fun(images, gt_masks, alpha_mattes, epsilon=1e-6):\n",
        "    la = tf.reduce_sum(tf.sqrt(tf.square(gt_masks - alpha_mattes) + epsilon))\n",
        "    lcolor = tf.reduce_sum(tf.sqrt(tf.square(tf.tile(gt_masks, multiples=(1,1,1,3)) * images\n",
        "                                             - tf.tile(alpha_mattes, multiples=(1,1,1,3)) * images) + epsilon))\n",
        "    return la + lcolor"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vcOBBz0YjItG",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def iou(real_B, fake_B):\n",
        "    real_B_ones = tf.greater_equal(real_B, 0.5)\n",
        "    fake_B_ones = tf.greater_equal(fake_B, 0.5)\n",
        "    i = tf.cast(tf.logical_and(real_B_ones, fake_B_ones), dtype=tf.float32)\n",
        "    u = tf.cast(tf.logical_or(real_B_ones, fake_B_ones), dtype=tf.float32)\n",
        "    iou = tf.reduce_mean(tf.reduce_sum(i, axis=[1, 2, 3]) / tf.reduce_sum(u, axis=[1, 2, 3]))\n",
        "    return iou"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "g_d35FWejcF7",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def segmentation_block(x):\n",
        "    x_shape = tf.shape(x)\n",
        "    out_w, out_h = x_shape[1], x_shape[2]\n",
        "    with tf.variable_scope(\"segmentation_block\", reuse=tf.AUTO_REUSE):\n",
        "        conv1 = conv(x, name=\"conv1\", filters=13, strides=2)\n",
        "        pool1 = tf.layers.max_pooling2d(x, pool_size=2, strides=2)\n",
        "        conv1_concat = tf.concat([conv1, pool1], axis=3)\n",
        "        conv2 = tf.nn.relu(conv(conv1_concat, name=\"conv2\", filters=16, dilation=2))\n",
        "        conv2_concat = tf.concat([conv1_concat, conv2], axis=3)\n",
        "        conv3 = tf.nn.relu(conv(conv2_concat, name=\"conv3\", filters=16, dilation=4))\n",
        "        conv3_concat = tf.concat([conv2_concat, conv3], axis=3)\n",
        "        conv4 = tf.nn.relu(conv(conv3_concat, name=\"conv4\", filters=16, dilation=6))\n",
        "        conv4_concat = tf.concat([conv3_concat, conv4], axis=3)\n",
        "        conv5 = tf.nn.relu(conv(conv4_concat, name=\"conv5\", filters=16, dilation=8))\n",
        "        conv5_concat = tf.concat([conv2, conv3, conv4, conv5], axis=3)\n",
        "        conv6 = tf.nn.relu(conv(conv5_concat, name=\"conv6\", filters=2))\n",
        "        pred = tf.image.resize_images(conv6, size=[out_w, out_h])\n",
        "    return pred"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "gK2JQC8Rjxsu",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def feathering_block(x, coarse_mask):\n",
        "    with tf.variable_scope(\"feathering_block\", reuse=tf.AUTO_REUSE):\n",
        "        foreground, background = tf.split(coarse_mask, axis=3, num_or_size_splits=2)\n",
        "        x_square = tf.square(x)\n",
        "        x_masked = x * tf.tile(foreground, multiples=(1,1,1,3))\n",
        "\n",
        "        x = tf.concat([x, coarse_mask, x_square, x_masked], axis=3)\n",
        "\n",
        "        conv1 = tf.nn.relu(instance_norm(conv(x, name=\"conv1\", filters=32), name=\"norm1\"))\n",
        "        conv4 = conv(conv1, name=\"conv4\", filters=3)\n",
        "\n",
        "        a, b, c = tf.split(conv4, axis=3, num_or_size_splits=3)\n",
        "\n",
        "        output = a * foreground + b * background + c\n",
        "    output = tf.nn.sigmoid(output)\n",
        "    return output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6j_byU5HkBHA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def _extract_features(example):\n",
        "    features = {\n",
        "        \"image\": tf.FixedLenFeature((), tf.string),\n",
        "        \"mask\": tf.FixedLenFeature((), tf.string)\n",
        "    }\n",
        "    parsed_example = tf.parse_single_example(example, features)\n",
        "    images = tf.image.decode_png(parsed_example[\"image\"],channels=3, dtype=tf.uint8)\n",
        "    images = tf.image.resize_images(images,[800, 600])\n",
        "    images=tf.reshape(images,[800,600,3])\n",
        "    masks = tf.image.decode_png(parsed_example[\"mask\"],channels=1, dtype=tf.uint8)\n",
        "    masks = tf.image.resize_images(masks,[800, 600])\n",
        "    masks=tf.reshape(masks,[800,600,1])\n",
        "    return images, masks\n",
        "\n",
        "\n",
        "def create_one_shot_iterator(filenames, batch_size, num_epoch):\n",
        "    dataset = tf.data.TFRecordDataset(filenames)\n",
        "    dataset = dataset.map(_extract_features)\n",
        "    dataset = dataset.shuffle(buffer_size=batch_size)\n",
        "    dataset = dataset.batch(batch_size)\n",
        "    dataset = dataset.repeat(num_epoch)\n",
        "    return dataset.make_one_shot_iterator()\n",
        "\n",
        "\n",
        "def create_initializable_iterator(filenames, batch_size):\n",
        "    dataset = tf.data.TFRecordDataset(filenames)\n",
        "    dataset = dataset.map(_extract_features)\n",
        "    dataset = dataset.shuffle(buffer_size=batch_size)\n",
        "    dataset = dataset.batch(batch_size)\n",
        "    return dataset.make_initializable_iterator()\n",
        "\n",
        "\n",
        "def augment_dataset(images, masks, size=None, augment=True):\n",
        "    if augment:\n",
        "        cond_flip_lr = tf.cast(tf.random_uniform([], maxval=2, dtype=tf.int32), tf.bool)\n",
        "        cond_rotate = tf.random_uniform([], minval=-1/6, maxval=1/6)\n",
        "\n",
        "        def orig(images, masks):\n",
        "            return images, masks\n",
        "\n",
        "        def flip(images, masks):\n",
        "            return tf.map_fn(tf.image.flip_left_right, images), tf.map_fn(tf.image.flip_left_right, masks)\n",
        "\n",
        "        images, masks = tf.cond(cond_flip_lr, lambda: flip(images, masks), lambda: orig(images, masks))\n",
        "\n",
        "        images, masks = tf.contrib.image.rotate(images, angles=cond_rotate), tf.contrib.image.rotate(masks, angles=cond_rotate)\n",
        "\n",
        "    if size is not None:\n",
        "        images = tf.image.resize_images(images, 1 * size)\n",
        "        masks = tf.image.resize_images(masks, 1 * size)\n",
        "\n",
        "    return images, masks"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "mk8TsWJz9_0t",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        ""
      ]
    },
    {
      "metadata": {
        "id": "z8LnaeIPkmLj",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "mode=1\n",
        "train_files=\"/content/drive/My Drive/Projetand/train-00001-of-00001\"\n",
        "test_files=\"/content/drive/My Drive/Projetand/test-00001-of-00001\"\n",
        "log_dir='/content/drive/My Drive/Projetand/log/'\n",
        "ckpt_dir='/content/drive/My Drive/Projetand/cpkt/'\n",
        "train_batch_size=20\n",
        "test_batch_size=2\n",
        "num_epochs=20000\n",
        "learning_rate=1e-3\n",
        "resume=None   \n",
        "\n",
        "\n",
        "\n",
        "   \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "dwG158IY84XM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "CNzec6pm5uWf",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        " \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6WZF-aNNdOhY",
        "colab_type": "code",
        "outputId": "986ecb0d-0e18-4009-c468-45252d0ae2d1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "cell_type": "code",
      "source": [
        " \n",
        "if mode is None or mode <= 0 or mode > 3:\n",
        "        raise Exception(\"Invalid mode\")\n",
        "\n",
        "\n",
        "    \n",
        "\n",
        "\n",
        "num_train_samples = sum(1 for f in file_io.get_matching_files(train_files) \n",
        "                            for n in tf.python_io.tf_record_iterator(f))\n",
        "\n",
        "num_test_samples = sum(1 for f in file_io.get_matching_files(test_files) \n",
        "                           for n in tf.python_io.tf_record_iterator(f))\n",
        "\n",
        "   \n",
        "\n",
        "train_iterator = create_one_shot_iterator(train_files, train_batch_size, num_epoch=num_epochs)\n",
        "test_iterator = create_initializable_iterator(test_files, batch_size=num_test_samples)\n",
        "\n",
        "next_images, next_masks = train_iterator.get_next()\n",
        "next_images, next_masks = augment_dataset(next_images, next_masks)\n",
        "coarse_masks = segmentation_block(next_images)\n",
        "alpha_mattes = feathering_block(next_images, coarse_masks)\n",
        "loss = loss_fun(next_images, next_masks, alpha_mattes)\n",
        "\n",
        "test_images, test_masks = test_iterator.get_next()\n",
        "test_images, test_masks = augment_dataset(test_images, test_masks,augment=False)\n",
        "test_coarse_masks = segmentation_block(test_images)\n",
        "test_alpha_mattes = feathering_block(test_images, test_coarse_masks)\n",
        "test_loss = loss_fun(test_images, test_masks, test_alpha_mattes)\n",
        "\n",
        "train_iou = iou(next_masks, alpha_mattes)\n",
        "test_iou = iou(test_masks, test_alpha_mattes)\n",
        "\n",
        "all_trainable_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES)\n",
        "\n",
        "train_op = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(loss, var_list=all_trainable_vars)\n",
        "\n",
        "summary = tf.summary.FileWriter(logdir=log_dir)\n",
        "image_summary = tf.summary.image(\"image\", next_images)\n",
        "gt_summary = tf.summary.image(\"gt\", next_masks * next_images)\n",
        "result_summary = tf.summary.image(\"result\", alpha_mattes * next_images)\n",
        "images_summary = tf.summary.merge([image_summary, gt_summary, result_summary])\n",
        "\n",
        "test_image_summary = tf.summary.image(\"test_image\", test_images)\n",
        "test_gt_summary = tf.summary.image(\"test_gt\", test_masks * test_images)\n",
        "test_result_summary = tf.summary.image(\"test_result\", test_alpha_mattes * test_images)\n",
        "test_images_summary = tf.summary.merge([test_image_summary, test_gt_summary, test_result_summary])\n",
        "\n",
        "loss_summary = tf.summary.scalar(\"loss\", loss)\n",
        "test_loss_summary = tf.summary.scalar(\"test_loss\", test_loss)\n",
        "\n",
        "train_iou_sum = tf.summary.scalar(\"train_iou\", train_iou)\n",
        "test_iou_sum = tf.summary.scalar(\"test_iou\", test_iou)\n",
        "\n",
        "saver = tf.train.Saver(var_list=tf.trainable_variables())\n",
        "\n",
        "  \n",
        "     \n",
        "\n",
        "def get_session(sess):\n",
        "    session = sess\n",
        "    while type(session).__name__ != 'Session':\n",
        "        session = session._sess\n",
        "    return session\n",
        "\n",
        "\n",
        "with tf.train.MonitoredTrainingSession() as sess:\n",
        " \n",
        "    it = 0\n",
        "    if resume is not None and resume > 0:\n",
        "        saver.restore(sess, os.path.join(ckpt_dir, \"ckpt\") + \"-{it}\".format(it=resume))\n",
        "        it = resume + 1\n",
        "\n",
        "    for it in range(300):\n",
        "        _, cur_loss, cur_images_summary, cur_loss_summary, cur_train_iou = sess.run([train_op, loss, images_summary, loss_summary, train_iou_sum])\n",
        "        summary.add_summary(cur_loss_summary, it)\n",
        "        summary.add_summary(cur_train_iou, it)\n",
        "\n",
        "        if it % 10 == 0:\n",
        "            summary.add_summary(cur_images_summary, it)\n",
        "            print(\"check2\")\n",
        "\n",
        "      \n",
        "\n",
        "        if it % 200 == 0:\n",
        "            ckpt_path = saver.save(get_session(sess), save_path=os.path.join(ckpt_dir),\n",
        "                                       write_meta_graph=False, global_step=it)\n",
        "            print(\"Checkpoint saved as: {ckpt_path}\".format(ckpt_path=ckpt_path))\n",
        "            print(\"check4\")\n",
        "\n",
        "        it += 1\n",
        "\n",
        "\n",
        "\n",
        "    ckpt_path = saver.save(get_session(sess), save_path=os.path.join(ckpt_dir), write_meta_graph=False,\n",
        "                               global_step=it)\n",
        "    print(\"Checkpoint saved as: {ckpt_path}\".format(ckpt_path=ckpt_path))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Graph was finalized.\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "EnN4Ovy-tuf4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}